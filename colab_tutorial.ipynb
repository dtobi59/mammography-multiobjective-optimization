{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Multi-Objective Hyperparameter Optimization for Breast Cancer Classification\n",
    "\n",
    "**Running on Google Colab**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dtobi59/mammography-multiobjective-optimization/blob/main/colab_tutorial.ipynb)\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Setup environment and clone repository\n",
    "2. Upload or mount datasets\n",
    "3. Run NSGA-III optimization with checkpointing\n",
    "4. Analyze results and visualize Pareto front\n",
    "5. Evaluate on source and target datasets\n",
    "\n",
    "**Author:** David ([@dtobi59](https://github.com/dtobi59))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Check GPU availability and clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "### Clone Repository from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dtobi59/mammography-multiobjective-optimization.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd mammography-multiobjective-optimization\n",
    "\n",
    "# List files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": "# Install required packages\n\\!pip install -q -r requirements.txt\n\n# Install the package in editable mode\n\\!pip install -q -e .\n\nprint(\"\n[SUCCESS] All dependencies installed\\!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install the package in editable mode to fix imports\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"\\n[SUCCESS] All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_setup"
   },
   "source": [
    "## 2. Dataset Setup\n",
    "\n",
    "**Option A: Use Small Demo Dataset** (Recommended for testing)\n",
    "\n",
    "**Option B: Upload Your Own Datasets**\n",
    "\n",
    "**Option C: Mount Google Drive** (Best for large datasets)\n",
    "\n",
    "Choose one option below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_a"
   },
   "source": [
    "### Option A: Create Demo Dataset (Quick Test)\n",
    "\n",
    "This creates a small synthetic dataset for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_demo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Create demo data directories\n",
    "os.makedirs(\"demo_data/vindr/images\", exist_ok=True)\n",
    "os.makedirs(\"demo_data/inbreast/images\", exist_ok=True)\n",
    "\n",
    "# Create demo VinDr-Mammo metadata\n",
    "vindr_data = []\n",
    "for patient_id in range(1, 6):  # 5 patients\n",
    "    for laterality in ['L', 'R']:\n",
    "        for view in ['CC', 'MLO']:\n",
    "            image_id = f\"P{patient_id:03d}_{laterality}_{view}\"\n",
    "            birads = np.random.choice([2, 3, 4, '4A', 5])\n",
    "            vindr_data.append({\n",
    "                'image_id': image_id,\n",
    "                'study_id': f'P{patient_id:03d}',\n",
    "                'laterality': laterality,\n",
    "                'view_position': view,\n",
    "                'breast_birads': birads\n",
    "            })\n",
    "            # Create dummy image (512x512 grayscale)\n",
    "            img = Image.fromarray(np.random.randint(0, 256, (512, 512), dtype=np.uint8), mode='L')\n",
    "            img.save(f\"demo_data/vindr/images/{image_id}.png\")\n",
    "\n",
    "vindr_df = pd.DataFrame(vindr_data)\n",
    "vindr_df.to_csv(\"demo_data/vindr/metadata.csv\", index=False)\n",
    "\n",
    "# Create demo INbreast metadata\n",
    "inbreast_data = []\n",
    "for patient_id in range(1, 4):  # 3 patients\n",
    "    for laterality in ['L', 'R']:\n",
    "        for view in ['CC', 'MLO']:\n",
    "            file_name = f\"INbreast_{patient_id:03d}_{laterality}_{view}.png\"\n",
    "            birads = np.random.choice([2, 3, '4B', 5])\n",
    "            inbreast_data.append({\n",
    "                'patient_id': f'INB{patient_id:03d}',\n",
    "                'laterality': laterality,\n",
    "                'view': view,\n",
    "                'birads': birads,\n",
    "                'file_name': file_name\n",
    "            })\n",
    "            # Create dummy image\n",
    "            img = Image.fromarray(np.random.randint(0, 256, (512, 512), dtype=np.uint8), mode='L')\n",
    "            img.save(f\"demo_data/inbreast/images/{file_name}\")\n",
    "\n",
    "inbreast_df = pd.DataFrame(inbreast_data)\n",
    "inbreast_df.to_csv(\"demo_data/inbreast/metadata.csv\", index=False)\n",
    "\n",
    "print(\"[SUCCESS] Demo dataset created!\")\n",
    "print(f\"VinDr-Mammo: {len(vindr_df)} images from 5 patients\")\n",
    "print(f\"INbreast: {len(inbreast_df)} images from 3 patients\")\n",
    "\n",
    "# Set paths for demo data\n",
    "VINDR_PATH = \"demo_data/vindr\"\n",
    "INBREAST_PATH = \"demo_data/inbreast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "### Option B: Upload Your Own Datasets\n",
    "\n",
    "Skip if using demo data or Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Uncomment to upload files\n",
    "# from google.colab import files\n",
    "#\n",
    "# print(\"Upload VinDr-Mammo metadata.csv\")\n",
    "# vindr_metadata = files.upload()\n",
    "#\n",
    "# print(\"Upload INbreast metadata.csv\")\n",
    "# inbreast_metadata = files.upload()\n",
    "#\n",
    "# # For images, it's better to use Google Drive for large datasets\n",
    "# print(\"For image files, please use Google Drive (Option C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_c"
   },
   "source": [
    "### Option C: Mount Google Drive\n",
    "\n",
    "Best option for large datasets. Upload your data to Google Drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Uncomment to mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "# # Set paths to your data in Google Drive\n",
    "# VINDR_PATH = \"/content/drive/MyDrive/datasets/vindr_mammo\"\n",
    "# INBREAST_PATH = \"/content/drive/MyDrive/datasets/inbreast\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configure Paths\n",
    "\n",
    "Update configuration with your dataset paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "update_config"
   },
   "outputs": [],
   "source": [
    "# Read current config\n",
    "with open('config.py', 'r') as f:\n",
    "    config_content = f.read()\n",
    "\n",
    "# Update paths (using demo data by default)\n",
    "config_content = config_content.replace(\n",
    "    'VINDR_MAMMO_PATH = \"/path/to/vindr_mammo\"',\n",
    "    f'VINDR_MAMMO_PATH = \"{VINDR_PATH}\"'\n",
    ")\n",
    "config_content = config_content.replace(\n",
    "    'INBREAST_PATH = \"/path/to/inbreast\"',\n",
    "    f'INBREAST_PATH = \"{INBREAST_PATH}\"'\n",
    ")\n",
    "\n",
    "# Reduce population size and generations for quick testing\n",
    "config_content = config_content.replace(\n",
    "    '\"pop_size\": 20,',\n",
    "    '\"pop_size\": 6,  # Reduced for demo'\n",
    ")\n",
    "config_content = config_content.replace(\n",
    "    '\"n_generations\": 100,',\n",
    "    '\"n_generations\": 5,  # Reduced for demo'\n",
    ")\n",
    "\n",
    "# Reduce epochs for faster training in demo\n",
    "config_content = config_content.replace(\n",
    "    'MAX_EPOCHS = 100',\n",
    "    'MAX_EPOCHS = 3  # Reduced for demo'\n",
    ")\n",
    "\n",
    "# Write updated config\n",
    "with open('config.py', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"[SUCCESS] Configuration updated!\")\n",
    "print(f\"VinDr-Mammo path: {VINDR_PATH}\")\n",
    "print(f\"INbreast path: {INBREAST_PATH}\")\n",
    "print(\"\\nDemo settings: pop_size=6, n_generations=5, max_epochs=3\")\n",
    "print(\"For production runs, increase these values in config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 4. Verify Setup\n",
    "\n",
    "Test that data loading works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_setup"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import config\n",
    "from optimization.nsga3_runner import load_metadata\n",
    "\n",
    "# Load VinDr-Mammo metadata\n",
    "print(\"Loading VinDr-Mammo metadata...\")\n",
    "vindr_metadata = load_metadata(\n",
    "    dataset_name=\"vindr\",\n",
    "    dataset_path=config.VINDR_MAMMO_PATH,\n",
    "    dataset_config=config.VINDR_CONFIG\n",
    ")\n",
    "print(f\"[OK] Loaded {len(vindr_metadata)} images\")\n",
    "print(f\"     Patients: {vindr_metadata['patient_id'].nunique()}\")\n",
    "print(f\"     Label distribution: {vindr_metadata['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Load INbreast metadata\n",
    "print(\"\\nLoading INbreast metadata...\")\n",
    "inbreast_metadata = load_metadata(\n",
    "    dataset_name=\"inbreast\",\n",
    "    dataset_path=config.INBREAST_PATH,\n",
    "    dataset_config=config.INBREAST_CONFIG\n",
    ")\n",
    "print(f\"[OK] Loaded {len(inbreast_metadata)} images\")\n",
    "print(f\"     Patients: {inbreast_metadata['patient_id'].nunique()}\")\n",
    "print(f\"     Label distribution: {inbreast_metadata['label'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n[SUCCESS] Setup verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimization"
   },
   "source": [
    "## 5. Run NSGA-III Optimization\n",
    "\n",
    "This will optimize 5 hyperparameters for 4 objectives with automatic checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_optimization"
   },
   "outputs": [],
   "source": [
    "from optimization.nsga3_runner import NSGA3Runner\n",
    "from data.dataset import create_train_val_split\n",
    "from pathlib import Path\n",
    "\n",
    "# Create train/val split\n",
    "print(\"Creating train/validation split...\")\n",
    "train_metadata, val_metadata = create_train_val_split(vindr_metadata)\n",
    "\n",
    "print(f\"Train samples: {len(train_metadata)}\")\n",
    "print(f\"Validation samples: {len(val_metadata)}\")\n",
    "print(f\"Unique patients - Train: {train_metadata['patient_id'].nunique()}, \"\n",
    "      f\"Val: {val_metadata['patient_id'].nunique()}\")\n",
    "\n",
    "# Create runner with checkpoint saving\n",
    "print(\"\\nInitializing NSGA-III runner...\")\n",
    "image_dir = str(Path(config.VINDR_MAMMO_PATH) / config.VINDR_CONFIG[\"image_dir\"])\n",
    "runner = NSGA3Runner(\n",
    "    train_metadata=train_metadata,\n",
    "    val_metadata=val_metadata,\n",
    "    image_dir=image_dir,\n",
    "    output_dir=\"./optimization_results\",\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    save_frequency=1  # Save every generation\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"This may take a while depending on:\")\n",
    "print(\"  - Population size (current: from config)\")\n",
    "print(\"  - Number of generations (current: from config)\")\n",
    "print(\"  - Dataset size\")\n",
    "print(\"  - GPU availability\")\n",
    "print(\"\\nCheckpoints will be saved every generation.\")\n",
    "print(\"You can monitor progress in the optimization_checkpoints folder.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run optimization\n",
    "result = runner.run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Pareto front size: {len(result.F)}\")\n",
    "print(f\"Results saved to: {runner.output_dir}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "checkpoints"
   },
   "source": [
    "## 6. Inspect Checkpoints\n",
    "\n",
    "View saved checkpoints and load Pareto fronts from different generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_checkpoints"
   },
   "outputs": [],
   "source": [
    "# List all checkpoints\n",
    "checkpoints = runner.list_checkpoints()\n",
    "print(f\"Found {len(checkpoints)} checkpoints:\\n\")\n",
    "\n",
    "for i, checkpoint_path in enumerate(checkpoints):\n",
    "    print(f\"{i+1}. {checkpoint_path.name}\")\n",
    "\n",
    "# Load and display the latest checkpoint\n",
    "if checkpoints:\n",
    "    print(\"\\nLoading latest checkpoint...\")\n",
    "    latest_checkpoint = runner.load_checkpoint(checkpoints[-1])\n",
    "    \n",
    "    # Get Pareto front from latest checkpoint\n",
    "    pareto_df = runner.get_pareto_front_from_checkpoint(checkpoints[-1])\n",
    "    print(f\"\\nPareto front at generation {latest_checkpoint['generation']}:\")\n",
    "    print(pareto_df)\n",
    "else:\n",
    "    print(\"No checkpoints found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analyze"
   },
   "source": [
    "## 7. Analyze Results\n",
    "\n",
    "Load and visualize the final Pareto front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_results"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Find most recent results file\n",
    "pareto_files = sorted(glob.glob(\"optimization_results/pareto_solutions_*.csv\"))\n",
    "if not pareto_files:\n",
    "    print(\"No results found. Please run optimization first.\")\n",
    "else:\n",
    "    latest_results = pareto_files[-1]\n",
    "    print(f\"Loading results from: {latest_results}\")\n",
    "    \n",
    "    results_df = pd.read_csv(latest_results)\n",
    "    print(f\"\\nPareto front contains {len(results_df)} solutions\")\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(results_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "### Visualize Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_pareto"
   },
   "outputs": [],
   "source": [
    "if pareto_files:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Pareto Front - Objective Space', fontsize=16)\n",
    "    \n",
    "    # Plot objective pairs\n",
    "    objective_pairs = [\n",
    "        ('pr_auc', 'auroc'),\n",
    "        ('pr_auc', 'brier'),\n",
    "        ('pr_auc', 'robustness_degradation'),\n",
    "        ('auroc', 'brier'),\n",
    "        ('auroc', 'robustness_degradation'),\n",
    "        ('brier', 'robustness_degradation')\n",
    "    ]\n",
    "    \n",
    "    for ax, (obj1, obj2) in zip(axes.flat, objective_pairs):\n",
    "        ax.scatter(results_df[obj1], results_df[obj2], alpha=0.6, s=50)\n",
    "        ax.set_xlabel(obj1.replace('_', ' ').title())\n",
    "        ax.set_ylabel(obj2.replace('_', ' ').title())\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot hyperparameter distributions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Pareto Front - Hyperparameter Distributions', fontsize=16)\n",
    "    \n",
    "    hyperparams = ['learning_rate', 'weight_decay', 'dropout_rate', \n",
    "                   'augmentation_strength', 'unfreeze_fraction']\n",
    "    \n",
    "    for ax, param in zip(axes.flat, hyperparams):\n",
    "        ax.hist(results_df[param], bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlabel(param.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide the last subplot (we have 5 params, 6 subplots)\n",
    "    axes.flat[-1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify extreme solutions\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTREME SOLUTIONS (Best for each objective)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best PR-AUC\n",
    "    best_pr_auc_idx = results_df['pr_auc'].idxmax()\n",
    "    print(f\"\\nBest PR-AUC: {results_df.loc[best_pr_auc_idx, 'pr_auc']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_pr_auc_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_pr_auc_idx, param]:.6f}\")\n",
    "    \n",
    "    # Best AUROC\n",
    "    best_auroc_idx = results_df['auroc'].idxmax()\n",
    "    print(f\"\\nBest AUROC: {results_df.loc[best_auroc_idx, 'auroc']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_auroc_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_auroc_idx, param]:.6f}\")\n",
    "    \n",
    "    # Best Brier (lowest)\n",
    "    best_brier_idx = results_df['brier'].idxmin()\n",
    "    print(f\"\\nBest Brier Score: {results_df.loc[best_brier_idx, 'brier']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_brier_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_brier_idx, param]:.6f}\")\n",
    "    \n",
    "    # Best Robustness (lowest degradation)\n",
    "    best_robust_idx = results_df['robustness_degradation'].idxmin()\n",
    "    print(f\"\\nBest Robustness: {results_df.loc[best_robust_idx, 'robustness_degradation']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_robust_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_robust_idx, param]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 8. Download Results\n",
    "\n",
    "Download optimization results, checkpoints, and trained models to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_results"
   },
   "outputs": [],
   "source": [
    "# Create zip file with all results\n",
    "!zip -r optimization_results.zip optimization_results/ checkpoints/\n",
    "\n",
    "print(\"\\n[SUCCESS] Results zipped!\")\n",
    "print(\"Download 'optimization_results.zip' using the file browser on the left.\")\n",
    "\n",
    "# Optionally, directly download using Colab files\n",
    "# from google.colab import files\n",
    "# files.download('optimization_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Evaluate on INbreast (Zero-Shot Transfer)**\n",
    "   - Use the best solution to evaluate on the target dataset\n",
    "   - No fine-tuning or threshold adjustment\n",
    "\n",
    "2. **Analyze Trade-offs**\n",
    "   - Compare different Pareto solutions\n",
    "   - Select based on your priorities (PR-AUC, AUROC, calibration, robustness)\n",
    "\n",
    "3. **Production Runs**\n",
    "   - Increase population size (e.g., 20)\n",
    "   - Increase generations (e.g., 100)\n",
    "   - Increase max epochs (e.g., 100)\n",
    "   - Use full datasets\n",
    "\n",
    "4. **Save to Google Drive**\n",
    "   - Mount Google Drive and save results there for persistence\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **GitHub Repository:** https://github.com/dtobi59/mammography-multiobjective-optimization\n",
    "- **Documentation:** See README.md and other guides in the repository\n",
    "- **Paper:** [Add your paper link here when published]\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this code in your research, please cite:\n",
    "\n",
    "```bibtex\n",
    "@software{mammography_multiobjective_optimization,\n",
    "  author = {David},\n",
    "  title = {Multi-Objective Hyperparameter Optimization for Breast Cancer Classification},\n",
    "  year = {2026},\n",
    "  url = {https://github.com/dtobi59/mammography-multiobjective-optimization}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}