{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Multi-Objective Hyperparameter Optimization for Breast Cancer Classification\n",
    "\n",
    "**Running on Google Colab**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dtobi59/mammography-multiobjective-optimization/blob/main/colab_tutorial.ipynb)\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. Setup environment and clone repository\n",
    "2. Upload or mount datasets\n",
    "3. Run NSGA-III optimization with checkpointing\n",
    "4. Analyze results and visualize Pareto front\n",
    "5. Evaluate on source and target datasets\n",
    "\n",
    "**Author:** David ([@dtobi59](https://github.com/dtobi59))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Check GPU availability and clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "### Clone Repository from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/dtobi59/mammography-multiobjective-optimization.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd mammography-multiobjective-optimization\n",
    "\n",
    "# List files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_python_path"
   },
   "outputs": [],
   "source": [
    "# Setup Python path to ensure imports work\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get current directory (project root)\n",
    "project_root = os.getcwd()\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add to Python path if not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "# Verify path setup\n",
    "print(f\"\\nPython sys.path[0]: {sys.path[0]}\")\n",
    "print(\"[OK] Path setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": "# Install required packages\n\\!pip install -q -r requirements.txt\n\n# Install the package in editable mode\n\\!pip install -q -e .\n\nprint(\"\n[SUCCESS] All dependencies installed\\!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\n[SUCCESS] All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_setup"
   },
   "source": "## 2. Dataset Setup\n\n**Option C: Mount Google Drive** (ACTIVE - Best for large datasets)\n\n**Option A: Use Small Demo Dataset** (For testing only - commented out)\n\n**Option B: Upload Your Own Datasets** (Alternative - commented out)\n\nWe'll use Google Drive by default to access the VinDr and INbreast datasets."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_a"
   },
   "source": "### Option A: Create Demo Dataset (Quick Test)\n\n**COMMENTED OUT - Using Google Drive instead**\n\nThis creates a small synthetic dataset for testing the pipeline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_demo"
   },
   "outputs": [],
   "source": "# OPTION A - COMMENTED OUT (Using Google Drive instead)\n# Uncomment this cell if you want to test with demo data instead\n\n# import os\n# import pandas as pd\n# import numpy as np\n# from PIL import Image\n#\n# # Create demo data directories\n# os.makedirs(\"demo_data/vindr/images\", exist_ok=True)\n# os.makedirs(\"demo_data/inbreast/images\", exist_ok=True)\n#\n# # Create demo VinDr-Mammo metadata\n# vindr_data = []\n# for patient_id in range(1, 6):  # 5 patients\n#     for laterality in ['L', 'R']:\n#         for view in ['CC', 'MLO']:\n#             image_id = f\"P{patient_id:03d}_{laterality}_{view}\"\n#             birads = np.random.choice([2, 3, 4, '4A', 5])\n#             vindr_data.append({\n#                 'image_id': image_id,\n#                 'study_id': f'P{patient_id:03d}',\n#                 'laterality': laterality,\n#                 'view_position': view,\n#                 'breast_birads': birads\n#             })\n#             # Create dummy image (512x512 grayscale)\n#             img = Image.fromarray(np.random.randint(0, 256, (512, 512), dtype=np.uint8), mode='L')\n#             img.save(f\"demo_data/vindr/images/{image_id}.png\")\n#\n# vindr_df = pd.DataFrame(vindr_data)\n# vindr_df.to_csv(\"demo_data/vindr/metadata.csv\", index=False)\n#\n# # Create demo INbreast metadata\n# inbreast_data = []\n# for patient_id in range(1, 4):  # 3 patients\n#     for laterality in ['L', 'R']:\n#         for view in ['CC', 'MLO']:\n#             file_name = f\"INbreast_{patient_id:03d}_{laterality}_{view}.png\"\n#             birads = np.random.choice([2, 3, '4B', 5])\n#             inbreast_data.append({\n#                 'patient_id': f'INB{patient_id:03d}',\n#                 'laterality': laterality,\n#                 'view': view,\n#                 'birads': birads,\n#                 'file_name': file_name\n#             })\n#             # Create dummy image\n#             img = Image.fromarray(np.random.randint(0, 256, (512, 512), dtype=np.uint8), mode='L')\n#             img.save(f\"demo_data/inbreast/images/{file_name}\")\n#\n# inbreast_df = pd.DataFrame(inbreast_data)\n# inbreast_df.to_csv(\"demo_data/inbreast/metadata.csv\", index=False)\n#\n# print(\"[SUCCESS] Demo dataset created!\")\n# print(f\"VinDr-Mammo: {len(vindr_df)} images from 5 patients\")\n# print(f\"INbreast: {len(inbreast_df)} images from 3 patients\")\n#\n# # Set paths for demo data\n# VINDR_PATH = \"demo_data/vindr\"\n# INBREAST_PATH = \"demo_data/inbreast\"\n\nprint(\"Option A is disabled. Using Google Drive (Option C).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "### Option B: Upload Your Own Datasets\n",
    "\n",
    "Skip if using demo data or Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Uncomment to upload files\n",
    "# from google.colab import files\n",
    "#\n",
    "# print(\"Upload VinDr-Mammo metadata.csv\")\n",
    "# vindr_metadata = files.upload()\n",
    "#\n",
    "# print(\"Upload INbreast metadata.csv\")\n",
    "# inbreast_metadata = files.upload()\n",
    "#\n",
    "# # For images, it's better to use Google Drive for large datasets\n",
    "# print(\"For image files, please use Google Drive (Option C)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_c"
   },
   "source": "### Option C: Mount Google Drive (ACTIVE)\n\n**This option is now active by default.**\n\nMake sure you have uploaded your datasets to Google Drive:\n- VinDr dataset: `/content/drive/MyDrive/kaggle_vindr_data/`\n- INbreast dataset: `/content/drive/MyDrive/INbreast/`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Set paths to your data in Google Drive\nVINDR_PATH = \"/content/drive/MyDrive/kaggle_vindr_data\"\nINBREAST_PATH = \"/content/drive/MyDrive/INbreast\"\n\n# Set checkpoint directories in Google Drive (persistent storage)\nCHECKPOINT_DIR = \"/content/drive/MyDrive/vindr_optimization/checkpoints\"\nOUTPUT_DIR = \"/content/drive/MyDrive/vindr_optimization/results\"\n\n# Create checkpoint directories\nimport os\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"\\n[SUCCESS] Google Drive mounted!\")\nprint(f\"VinDr dataset path: {VINDR_PATH}\")\nprint(f\"INbreast dataset path: {INBREAST_PATH}\")\nprint(f\"\\nCheckpoints will be saved to:\")\nprint(f\"  Model checkpoints: {CHECKPOINT_DIR}\")\nprint(f\"  Optimization results: {OUTPUT_DIR}\")\nprint(f\"\\n‚úì Checkpoints are persistent and won't be lost when session ends!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configure Paths\n",
    "\n",
    "Update configuration with your dataset paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "update_config"
   },
   "outputs": [],
   "source": "# Read current config\nwith open('config.py', 'r') as f:\n    config_content = f.read()\n\n# Update paths to Google Drive locations\nconfig_content = config_content.replace(\n    'VINDR_MAMMO_PATH = \"/content/drive/MyDrive/kaggle_vindr_data\"',\n    f'VINDR_MAMMO_PATH = \"{VINDR_PATH}\"'\n)\nconfig_content = config_content.replace(\n    'INBREAST_PATH = \"/content/drive/MyDrive/INbreast\"',\n    f'INBREAST_PATH = \"{INBREAST_PATH}\"'\n)\n\n# Optional: Reduce for testing (uncomment if needed)\n# config_content = config_content.replace(\n#     '\"pop_size\": 24,',\n#     '\"pop_size\": 6,  # Reduced for testing'\n# )\n# config_content = config_content.replace(\n#     '\"n_generations\": 50,',\n#     '\"n_generations\": 5,  # Reduced for testing'\n# )\n# config_content = config_content.replace(\n#     'MAX_EPOCHS = 100',\n#     'MAX_EPOCHS = 10  # Reduced for testing'\n# )\n\n# Write updated config\nwith open('config.py', 'w') as f:\n    f.write(config_content)\n\nprint(\"[SUCCESS] Configuration updated!\")\nprint(f\"VinDr-Mammo path: {VINDR_PATH}\")\nprint(f\"INbreast path: {INBREAST_PATH}\")\nprint(\"\\nUsing production settings from config.py\")\nprint(\"To reduce compute time, uncomment the optional section above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 4. Verify Setup\n",
    "\n",
    "Test that data loading works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_setup"
   },
   "outputs": [],
   "source": [
    "# Verify setup and test data loading\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure project root is in path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path includes project root: {project_root in sys.path}\\n\")\n",
    "\n",
    "# Now import modules\n",
    "import config\n",
    "from optimization.nsga3_runner import load_metadata\n",
    "\n",
    "# Load VinDr-Mammo metadata\n",
    "print(\"Loading VinDr-Mammo metadata...\")\n",
    "vindr_metadata = load_metadata(\n",
    "    dataset_name=\"vindr\",\n",
    "    dataset_path=config.VINDR_MAMMO_PATH,\n",
    "    dataset_config=config.VINDR_CONFIG\n",
    ")\n",
    "print(f\"[OK] Loaded {len(vindr_metadata)} images\")\n",
    "print(f\"     Patients: {vindr_metadata['patient_id'].nunique()}\")\n",
    "print(f\"     Label distribution: {vindr_metadata['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Load INbreast metadata\n",
    "print(\"\\nLoading INbreast metadata...\")\n",
    "inbreast_metadata = load_metadata(\n",
    "    dataset_name=\"inbreast\",\n",
    "    dataset_path=config.INBREAST_PATH,\n",
    "    dataset_config=config.INBREAST_CONFIG\n",
    ")\n",
    "print(f\"[OK] Loaded {len(inbreast_metadata)} images\")\n",
    "print(f\"     Patients: {inbreast_metadata['patient_id'].nunique()}\")\n",
    "print(f\"     Label distribution: {inbreast_metadata['label'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n[SUCCESS] Setup verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "optimization"
   },
   "source": [
    "## 5. Run NSGA-III Optimization\n",
    "\n",
    "This will optimize 5 hyperparameters for 4 objectives with automatic checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_optimization"
   },
   "outputs": [],
   "source": "# Ensure imports work\nimport sys\nimport os\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nfrom optimization.nsga3_runner import NSGA3Runner\nfrom data.dataset import create_train_val_split\nfrom pathlib import Path\nimport config\n\n# Create train/val split\nprint(\"Creating train/validation split...\")\ntrain_metadata, val_metadata = create_train_val_split(vindr_metadata)\n\nprint(f\"Train samples: {len(train_metadata)}\")\nprint(f\"Validation samples: {len(val_metadata)}\")\nprint(f\"Unique patients - Train: {train_metadata['patient_id'].nunique()}, \"\n      f\"Val: {val_metadata['patient_id'].nunique()}\")\n\n# Create runner with checkpoint saving to Google Drive\nprint(\"\\nInitializing NSGA-III runner...\")\nimage_dir = str(Path(config.VINDR_MAMMO_PATH) / config.VINDR_CONFIG[\"image_dir\"])\nrunner = NSGA3Runner(\n    train_metadata=train_metadata,\n    val_metadata=val_metadata,\n    image_dir=image_dir,\n    output_dir=OUTPUT_DIR,          # Save to Google Drive\n    checkpoint_dir=CHECKPOINT_DIR,  # Save to Google Drive\n    save_frequency=1  # Save every generation\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING OPTIMIZATION\")\nprint(\"=\"*80)\nprint(\"This may take a while depending on:\")\nprint(\"  - Population size (current: from config)\")\nprint(\"  - Number of generations (current: from config)\")\nprint(\"  - Dataset size\")\nprint(\"  - GPU availability\")\nprint(\"\\nCheckpoints are being saved to Google Drive:\")\nprint(f\"  {CHECKPOINT_DIR}\")\nprint(f\"  {OUTPUT_DIR}\")\nprint(\"\\n‚úì Your progress is safe and persistent!\")\nprint(\"=\"*80 + \"\\n\")\n\n# Run optimization\nresult = runner.run()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"OPTIMIZATION COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"Pareto front size: {len(result.F)}\")\nprint(f\"Results saved to: {runner.output_dir}\")\nprint(f\"\\n‚úì All checkpoints and results are saved in Google Drive!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "checkpoints"
   },
   "source": [
    "## 6. Inspect Checkpoints\n",
    "\n",
    "View saved checkpoints and load Pareto fronts from different generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_checkpoints"
   },
   "outputs": [],
   "source": [
    "# List all checkpoints\n",
    "checkpoints = runner.list_checkpoints()\n",
    "print(f\"Found {len(checkpoints)} checkpoints:\\n\")\n",
    "\n",
    "for i, checkpoint_path in enumerate(checkpoints):\n",
    "    print(f\"{i+1}. {checkpoint_path.name}\")\n",
    "\n",
    "# Load and display the latest checkpoint\n",
    "if checkpoints:\n",
    "    print(\"\\nLoading latest checkpoint...\")\n",
    "    latest_checkpoint = runner.load_checkpoint(checkpoints[-1])\n",
    "    \n",
    "    # Get Pareto front from latest checkpoint\n",
    "    pareto_df = runner.get_pareto_front_from_checkpoint(checkpoints[-1])\n",
    "    print(f\"\\nPareto front at generation {latest_checkpoint['generation']}:\")\n",
    "    print(pareto_df)\n",
    "else:\n",
    "    print(\"No checkpoints found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analyze"
   },
   "source": [
    "## 7. Analyze Results\n",
    "\n",
    "Load and visualize the final Pareto front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_results"
   },
   "outputs": [],
   "source": "import glob\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Find most recent results file in Google Drive\npareto_files = sorted(glob.glob(f\"{OUTPUT_DIR}/pareto_solutions_*.csv\"))\nif not pareto_files:\n    print(\"No results found. Please run optimization first.\")\nelse:\n    latest_results = pareto_files[-1]\n    print(f\"Loading results from: {latest_results}\")\n    \n    results_df = pd.read_csv(latest_results)\n    print(f\"\\nPareto front contains {len(results_df)} solutions\")\n    print(\"\\nSummary statistics:\")\n    print(results_df.describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize"
   },
   "source": [
    "### Visualize Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_pareto"
   },
   "outputs": [],
   "source": [
    "if pareto_files:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Pareto Front - Objective Space', fontsize=16)\n",
    "    \n",
    "    # Plot objective pairs\n",
    "    objective_pairs = [\n",
    "        ('pr_auc', 'auroc'),\n",
    "        ('pr_auc', 'brier'),\n",
    "        ('pr_auc', 'robustness_degradation'),\n",
    "        ('auroc', 'brier'),\n",
    "        ('auroc', 'robustness_degradation'),\n",
    "        ('brier', 'robustness_degradation')\n",
    "    ]\n",
    "    \n",
    "    for ax, (obj1, obj2) in zip(axes.flat, objective_pairs):\n",
    "        ax.scatter(results_df[obj1], results_df[obj2], alpha=0.6, s=50)\n",
    "        ax.set_xlabel(obj1.replace('_', ' ').title())\n",
    "        ax.set_ylabel(obj2.replace('_', ' ').title())\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot hyperparameter distributions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Pareto Front - Hyperparameter Distributions', fontsize=16)\n",
    "    \n",
    "    hyperparams = ['learning_rate', 'weight_decay', 'dropout_rate', \n",
    "                   'augmentation_strength', 'unfreeze_fraction']\n",
    "    \n",
    "    for ax, param in zip(axes.flat, hyperparams):\n",
    "        ax.hist(results_df[param], bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlabel(param.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide the last subplot (we have 5 params, 6 subplots)\n",
    "    axes.flat[-1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify extreme solutions\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXTREME SOLUTIONS (Best for each objective)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best PR-AUC\n",
    "    best_pr_auc_idx = results_df['pr_auc'].idxmax()\n",
    "    print(f\"\\nBest PR-AUC: {results_df.loc[best_pr_auc_idx, 'pr_auc']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_pr_auc_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_pr_auc_idx, param]:.6f}\")\n",
    "    \n",
    "    # Best AUROC\n",
    "    best_auroc_idx = results_df['auroc'].idxmax()\n",
    "    print(f\"\\nBest AUROC: {results_df.loc[best_auroc_idx, 'auroc']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_auroc_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_auroc_idx, param]:.6f}\")\n",
    "    \n",
    "    # Best Brier (lowest)\n",
    "    best_brier_idx = results_df['brier'].idxmin()\n",
    "    print(f\"\\nBest Brier Score: {results_df.loc[best_brier_idx, 'brier']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_brier_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_brier_idx, param]:.6f}\")\n",
    "    \n",
    "    # Best Robustness (lowest degradation)\n",
    "    best_robust_idx = results_df['robustness_degradation'].idxmin()\n",
    "    print(f\"\\nBest Robustness: {results_df.loc[best_robust_idx, 'robustness_degradation']:.4f}\")\n",
    "    print(f\"Solution ID: {results_df.loc[best_robust_idx, 'solution_id']}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for param in hyperparams:\n",
    "        print(f\"  {param}: {results_df.loc[best_robust_idx, param]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": "## 8. Results in Google Drive\n\nAll results and checkpoints are automatically saved to Google Drive!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_results"
   },
   "outputs": [],
   "source": "# Results are already in Google Drive!\nprint(\"=\" * 80)\nprint(\"RESULTS LOCATION\")\nprint(\"=\" * 80)\nprint(f\"\\nAll results are saved in Google Drive:\")\nprint(f\"  üìÅ {OUTPUT_DIR}\")\nprint(f\"\\nCheckpoint structure:\")\nprint(f\"  ‚îú‚îÄ‚îÄ optimization_checkpoints/\")\nprint(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_gen_0001.pkl\")\nprint(f\"  ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_gen_0002.pkl\")\nprint(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ pareto_gen_XXXX.csv\")\nprint(f\"  ‚îî‚îÄ‚îÄ pareto_solutions_TIMESTAMP.csv\")\nprint(f\"\\nModel checkpoints:\")\nprint(f\"  üìÅ {CHECKPOINT_DIR}\")\nprint(f\"  ‚îú‚îÄ‚îÄ eval_1/best_checkpoint.pt\")\nprint(f\"  ‚îú‚îÄ‚îÄ eval_2/best_checkpoint.pt\")\nprint(f\"  ‚îî‚îÄ‚îÄ ...\")\nprint(\"\\n‚úì Access these files anytime from your Google Drive!\")\nprint(\"=\" * 80)\n\n# Optional: Create a zip file for download\nprint(\"\\nOptional: Create zip file for download\")\nprint(\"Uncomment the code below if you want to download results:\")\nprint()\nprint(\"# !cd /content/drive/MyDrive/vindr_optimization && zip -r results.zip results/ checkpoints/\")\nprint(\"# from google.colab import files\")\nprint(\"# files.download('/content/drive/MyDrive/vindr_optimization/results.zip')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": "## Next Steps\n\n1. **Evaluate on INbreast (Zero-Shot Transfer)**\n   - Use the best solution to evaluate on the target dataset\n   - No fine-tuning or threshold adjustment\n\n2. **Analyze Trade-offs**\n   - Compare different Pareto solutions\n   - Select based on your priorities (PR-AUC, AUROC, calibration, robustness)\n\n3. **Production Runs**\n   - Increase population size (e.g., 20)\n   - Increase generations (e.g., 100)\n   - Increase max epochs (e.g., 100)\n   - Use full datasets\n\n4. **Resume Training**\n   - All checkpoints are saved in Google Drive\n   - You can resume optimization from any checkpoint\n   - Simply load and continue training\n\n## Access Your Results\n\nAll files are stored in Google Drive:\n- **Results:** `/content/drive/MyDrive/vindr_optimization/results/`\n- **Checkpoints:** `/content/drive/MyDrive/vindr_optimization/checkpoints/`\n\nYou can access these from any Colab session or download them to your computer!\n\n## Resources\n\n- **GitHub Repository:** https://github.com/dtobi59/mammography-multiobjective-optimization\n- **Documentation:** See README.md and other guides in the repository\n- **Paper:** [Add your paper link here when published]\n\n## Citation\n\nIf you use this code in your research, please cite:\n\n```bibtex\n@software{mammography_multiobjective_optimization,\n  author = {David},\n  title = {Multi-Objective Hyperparameter Optimization for Breast Cancer Classification},\n  year = {2026},\n  url = {https://github.com/dtobi59/mammography-multiobjective-optimization}\n}\n```"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}